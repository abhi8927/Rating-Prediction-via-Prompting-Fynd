{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1: Rating Prediction via Prompting\n",
        "\n",
        "This notebook implements three different prompting approaches to classify Yelp reviews into star ratings (1-5) using Google Gemini API.\n",
        "\n",
        "## Objectives\n",
        "- Implement 3 different prompting approaches\n",
        "- Evaluate accuracy, JSON validity, and consistency\n",
        "- Compare results across approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "import os\n",
        "from typing import List, Dict\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "env_path = os.path.join(os.path.dirname(os.getcwd()), 'task1', '.env')\n",
        "if os.path.exists(env_path):\n",
        "    try:\n",
        "        load_dotenv(env_path)\n",
        "    except:\n",
        "        try:\n",
        "            with open(env_path, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    if line.startswith('GEMINI_API_KEY='):\n",
        "                        os.environ['GEMINI_API_KEY'] = line.split('=', 1)[1].strip()\n",
        "                        break\n",
        "        except:\n",
        "            pass\n",
        "else:\n",
        "    load_dotenv()\n",
        "\n",
        "# Add paths for imports\n",
        "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
        "from prompts.prompt_versions import get_direct_classification_prompt, get_few_shot_prompt, get_chain_of_thought_prompt\n",
        "from utils.evaluation import evaluate_approach, calculate_accuracy, calculate_json_validity_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure Gemini API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure Gemini API\n",
        "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
        "if not GEMINI_API_KEY:\n",
        "    # Fallback: use the API key directly\n",
        "    GEMINI_API_KEY = 'AIzaSyDnfybUacyg2A4WqPR7GjhuLVY00r18xh4'\n",
        "\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "print(\"Gemini API configured successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_yelp_data(file_path: str, sample_size: int = 200) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Loads and samples the Yelp reviews dataset.\n",
        "    \"\"\"\n",
        "    print(f\"Loading data from {file_path}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Total rows in dataset: {len(df)}\")\n",
        "        \n",
        "        # Sample the data\n",
        "        if len(df) > sample_size:\n",
        "            df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
        "            print(f\"Sampled {sample_size} reviews for evaluation\")\n",
        "        else:\n",
        "            print(f\"Using all {len(df)} reviews (less than sample size)\")\n",
        "        \n",
        "        # Verify required columns exist\n",
        "        if 'text' not in df.columns or 'stars' not in df.columns:\n",
        "            print(\"Error: Dataset must contain 'text' and 'stars' columns\")\n",
        "            print(f\"Available columns: {df.columns.tolist()}\")\n",
        "            return pd.DataFrame()\n",
        "        \n",
        "        print(f\"Successfully loaded {len(df)} reviews\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File {file_path} not found.\")\n",
        "        print(\"Please ensure yelp.csv is in the data/ directory\")\n",
        "        return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Load the dataset\n",
        "data_file = \"data/yelp.csv\"\n",
        "df = load_yelp_data(data_file)\n",
        "\n",
        "if not df.empty:\n",
        "    print(f\"\\nDataset preview:\")\n",
        "    print(df.head())\n",
        "    print(f\"\\nDataset shape: {df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompting Approaches\n",
        "\n",
        "We implement three different prompting strategies:\n",
        "\n",
        "1. **Direct Classification**: Simple instruction-based prompt\n",
        "2. **Few-Shot Learning**: Includes examples in the prompt\n",
        "3. **Chain-of-Thought**: Step-by-step reasoning approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Approach 1: Direct Classification\n",
        "\n",
        "This is the simplest approach - directly asking the model to classify the review."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of Direct Classification prompt\n",
        "example_review = \"Amazing food and great service! Highly recommend.\"\n",
        "example_prompt = get_direct_classification_prompt(example_review)\n",
        "print(\"Example Direct Classification Prompt:\")\n",
        "print(\"=\" * 60)\n",
        "print(example_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Approach 2: Few-Shot Learning\n",
        "\n",
        "This approach provides examples to guide the model's understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of Few-Shot prompt\n",
        "example_prompt = get_few_shot_prompt(example_review)\n",
        "print(\"Example Few-Shot Learning Prompt:\")\n",
        "print(\"=\" * 60)\n",
        "print(example_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Approach 3: Chain-of-Thought\n",
        "\n",
        "This approach asks the model to reason through the classification step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of Chain-of-Thought prompt\n",
        "example_prompt = get_chain_of_thought_prompt(example_review)\n",
        "print(\"Example Chain-of-Thought Prompt:\")\n",
        "print(\"=\" * 60)\n",
        "print(example_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_rating(prompt: str) -> str:\n",
        "    \"\"\"\n",
        "    Sends prompt to Gemini API and returns response.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error calling API: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def run_evaluation(df: pd.DataFrame, approach_name: str, prompt_func) -> Dict:\n",
        "    \"\"\"\n",
        "    Runs evaluation for a single prompting approach.\n",
        "    \"\"\"\n",
        "    print(f\"\\nEvaluating {approach_name} approach...\")\n",
        "    responses = []\n",
        "    actual_ratings = []\n",
        "    \n",
        "    for idx, row in df.iterrows():\n",
        "        review_text = str(row.get('text', ''))\n",
        "        actual_rating = int(row.get('stars', 3))\n",
        "        \n",
        "        # Skip if review text is empty or invalid\n",
        "        if not review_text or review_text == 'nan' or len(review_text.strip()) == 0:\n",
        "            continue\n",
        "        \n",
        "        prompt = prompt_func(review_text)\n",
        "        response = predict_rating(prompt)\n",
        "        \n",
        "        responses.append(response)\n",
        "        actual_ratings.append(actual_rating)\n",
        "        \n",
        "        if (idx + 1) % 20 == 0:\n",
        "            print(f\"Processed {idx + 1}/{len(df)} reviews...\")\n",
        "    \n",
        "    results = evaluate_approach(responses, actual_ratings)\n",
        "    results['approach_name'] = approach_name\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if df.empty:\n",
        "    print(\"Dataset is empty. Please ensure yelp.csv is in the data/ directory\")\n",
        "else:\n",
        "    # Define approaches\n",
        "    approaches = [\n",
        "        (\"Direct Classification\", get_direct_classification_prompt),\n",
        "        (\"Few-Shot Learning\", get_few_shot_prompt),\n",
        "        (\"Chain-of-Thought\", get_chain_of_thought_prompt)\n",
        "    ]\n",
        "    \n",
        "    # Run evaluations\n",
        "    all_results = []\n",
        "    for approach_name, prompt_func in approaches:\n",
        "        results = run_evaluation(df, approach_name, prompt_func)\n",
        "        all_results.append(results)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"All evaluations completed!\")\n",
        "    print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results and Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print comparison table\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"COMPARISON TABLE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"{'Approach':<25} {'Accuracy':<12} {'JSON Validity':<15} {'Valid Predictions':<20}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for result in all_results:\n",
        "    print(f\"{result['approach_name']:<25} \"\n",
        "          f\"{result['accuracy']:.2f}%{'':<8} \"\n",
        "          f\"{result['json_validity_rate']:.2f}%{'':<10} \"\n",
        "          f\"{result['valid_predictions_count']:<20}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion and Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DISCUSSION\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nKey Findings:\")\n",
        "print(\"1. Direct Classification: Simple and fast, but may lack context.\")\n",
        "print(\"2. Few-Shot Learning: Provides examples to guide the model.\")\n",
        "print(\"3. Chain-of-Thought: Encourages step-by-step reasoning.\")\n",
        "print(\"\\nTrade-offs:\")\n",
        "print(\"- More complex prompts may improve accuracy but increase API costs.\")\n",
        "print(\"- JSON validity is crucial for production use.\")\n",
        "print(\"- Consistency across runs indicates reliability.\")\n",
        "print(\"\\nRecommendations:\")\n",
        "print(\"- For production: Choose approach with best balance of accuracy and JSON validity\")\n",
        "print(\"- For cost optimization: Consider simpler prompts if accuracy difference is minimal\")\n",
        "print(\"- For reliability: Test consistency across multiple runs\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
